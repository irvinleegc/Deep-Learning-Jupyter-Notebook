{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import all necessary packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.8\n",
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testing_generator():\n",
    "    xy=[]\n",
    "    zz=[]\n",
    "    excl = [1, 2, 3]\n",
    "    for i in range(1000):\n",
    "        while x in excl:\n",
    "            x = random.randint(0, 9)\n",
    "        while y in excl:\n",
    "            y = random.randint(0, 9)\n",
    "        z = x+y\n",
    "        xy = np.append(xy,[x,y])\n",
    "        zz = np.append(zz,[z])\n",
    "\n",
    "#print(x, y, xy)\n",
    "#print(z, zz) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training size and testing data size\n",
    "train_size = 500000\n",
    "test_size = 20000\n",
    "\n",
    "# generate random number for training data\n",
    "xy_train=[]\n",
    "zz_train=[]\n",
    "excl_train = [10, 33, 55, 27, 79]\n",
    "\n",
    "xy_test=[]\n",
    "zz_test=[]\n",
    "excl_test = [200] #200 is definately not in range, do this to kick start the generator.\n",
    "\n",
    "def to_one_hot(labels, dimension=201):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        #print(int(label))\n",
    "        results[i, int(label)] = 1.\n",
    "    return results\n",
    "\n",
    "def data_generator(xy, zz, exclude, size):\n",
    "    for i in range(size):\n",
    "        x = exclude[0]\n",
    "        y = exclude[0]\n",
    "        #z=[201]\n",
    "        z=[]\n",
    "        #print(x, y)\n",
    "        while x in exclude:\n",
    "            x = random.randint(0, 100)\n",
    "        while y in exclude:\n",
    "            y = random.randint(0, 100)\n",
    "        z = x+y\n",
    "        xy = np.append(xy,[x,y])\n",
    "        zz = np.append(zz,[z])\n",
    "        \n",
    "    xy = np.reshape(xy, (size,2))\n",
    "    zz = to_one_hot(zz,201)\n",
    "    #print(zz)  \n",
    "    return (xy, zz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 2)\n",
      "(500000, 201)\n"
     ]
    }
   ],
   "source": [
    "#generate training data\n",
    "(xy_train, zz_train) = data_generator(xy_train, zz_train ,excl_train, train_size)\n",
    "\n",
    "#generate testing data\n",
    "(xy_test, zz_test) = data_generator(xy_test, zz_test ,excl_test, test_size)\n",
    "\n",
    "#nomarlize the data to 0 to 1\n",
    "#xy_train = xy_train/101\n",
    "#zz_train = zz_train/101\n",
    "\n",
    "#print(xy_train)\n",
    "print(xy_train.shape)\n",
    "#print(zz_train)\n",
    "print(zz_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the training parameters\n",
    "learning_rate = 0.1 \n",
    "training_epochs = 100\n",
    "batch_size = 1000\n",
    "\n",
    "# define the network parameters\n",
    "n_input = 2 # for 2 numbers (0-100) to be added\n",
    "n_hidden_1 = 100 # 1st layer number of neurons\n",
    "n_hidden_2 = 100 # 2nd layer number of neurons\n",
    "n_classes = 201\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 100)               300       \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "Outputlayer (Dense)          (None, 201)               20301     \n",
      "=================================================================\n",
      "Total params: 30,701\n",
      "Trainable params: 30,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the network layers\n",
    "Inp = Input(shape=(n_input,))\n",
    "x = Dense(n_hidden_1, activation='relu', name = \"Dense_1\")(Inp)\n",
    "x = Dense(n_hidden_2, activation='relu', name = \"Dense_2\")(x)\n",
    "output = Dense(n_classes, activation='softmax', name = \"Outputlayer\")(x)\n",
    "\n",
    "model = Model(Inp, output)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# complie losses\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500000 samples, validate on 20000 samples\n",
      "Epoch 1/100\n",
      "500000/500000 [==============================] - 23s - loss: 4.8942 - acc: 0.0173 - val_loss: 3.9426 - val_acc: 0.0452\n",
      "Epoch 2/100\n",
      "500000/500000 [==============================] - 23s - loss: 3.4959 - acc: 0.0851 - val_loss: 3.1798 - val_acc: 0.1219\n",
      "Epoch 3/100\n",
      "500000/500000 [==============================] - 23s - loss: 2.9900 - acc: 0.1413 - val_loss: 2.8288 - val_acc: 0.1626\n",
      "Epoch 4/100\n",
      "500000/500000 [==============================] - 23s - loss: 2.7155 - acc: 0.1794 - val_loss: 2.6046 - val_acc: 0.2036\n",
      "Epoch 5/100\n",
      "500000/500000 [==============================] - 23s - loss: 2.5360 - acc: 0.2107 - val_loss: 2.4688 - val_acc: 0.2017\n",
      "Epoch 6/100\n",
      "500000/500000 [==============================] - 23s - loss: 2.4049 - acc: 0.2332 - val_loss: 2.3370 - val_acc: 0.2530\n",
      "Epoch 7/100\n",
      "500000/500000 [==============================] - 22s - loss: 2.2952 - acc: 0.2579 - val_loss: 2.2400 - val_acc: 0.2641\n",
      "Epoch 8/100\n",
      "500000/500000 [==============================] - 23s - loss: 2.2083 - acc: 0.2745 - val_loss: 2.1585 - val_acc: 0.2874\n",
      "Epoch 9/100\n",
      "500000/500000 [==============================] - 22s - loss: 2.1275 - acc: 0.2954 - val_loss: 2.0811 - val_acc: 0.3137\n",
      "Epoch 10/100\n",
      "500000/500000 [==============================] - 22s - loss: 2.0593 - acc: 0.3108 - val_loss: 2.0217 - val_acc: 0.3132\n",
      "Epoch 11/100\n",
      "500000/500000 [==============================] - 23s - loss: 1.9946 - acc: 0.3249 - val_loss: 1.9557 - val_acc: 0.3308\n",
      "Epoch 12/100\n",
      "500000/500000 [==============================] - 22s - loss: 1.9355 - acc: 0.3405 - val_loss: 1.9084 - val_acc: 0.3342\n",
      "Epoch 13/100\n",
      "500000/500000 [==============================] - 22s - loss: 1.8841 - acc: 0.3525 - val_loss: 1.8627 - val_acc: 0.3726\n",
      "Epoch 14/100\n",
      "500000/500000 [==============================] - 23s - loss: 1.8316 - acc: 0.3667 - val_loss: 1.8190 - val_acc: 0.3850\n",
      "Epoch 15/100\n",
      "500000/500000 [==============================] - 22s - loss: 1.7855 - acc: 0.3779 - val_loss: 1.8015 - val_acc: 0.3378\n",
      "Epoch 16/100\n",
      "500000/500000 [==============================] - 22s - loss: 1.7358 - acc: 0.3882 - val_loss: 1.7255 - val_acc: 0.3883\n",
      "Epoch 17/100\n",
      "500000/500000 [==============================] - 24s - loss: 1.6843 - acc: 0.4053 - val_loss: 1.7046 - val_acc: 0.3700\n",
      "Epoch 18/100\n",
      "500000/500000 [==============================] - 24s - loss: 1.6484 - acc: 0.4152 - val_loss: 1.6282 - val_acc: 0.4217\n",
      "Epoch 19/100\n",
      "500000/500000 [==============================] - 23s - loss: 1.6130 - acc: 0.4247 - val_loss: 1.6657 - val_acc: 0.3959\n",
      "Epoch 20/100\n",
      "500000/500000 [==============================] - 23s - loss: 1.5789 - acc: 0.4351 - val_loss: 1.5452 - val_acc: 0.4826\n",
      "Epoch 21/100\n",
      "500000/500000 [==============================] - 22s - loss: 1.5514 - acc: 0.4407 - val_loss: 1.5112 - val_acc: 0.4712\n",
      "Epoch 22/100\n",
      "500000/500000 [==============================] - 23s - loss: 1.5211 - acc: 0.4522 - val_loss: 1.5162 - val_acc: 0.4113\n",
      "Epoch 23/100\n",
      "500000/500000 [==============================] - 23s - loss: 1.4930 - acc: 0.4622 - val_loss: 1.4972 - val_acc: 0.4996\n",
      "Epoch 24/100\n",
      "500000/500000 [==============================] - 22s - loss: 1.4673 - acc: 0.4688 - val_loss: 1.4560 - val_acc: 0.4881\n",
      "Epoch 25/100\n",
      "500000/500000 [==============================] - 22s - loss: 1.4425 - acc: 0.4758 - val_loss: 1.5398 - val_acc: 0.3891\n",
      "Epoch 26/100\n",
      "500000/500000 [==============================] - 22s - loss: 1.4211 - acc: 0.4838 - val_loss: 1.4021 - val_acc: 0.5014\n",
      "Epoch 27/100\n",
      "500000/500000 [==============================] - 22s - loss: 1.3901 - acc: 0.4919 - val_loss: 1.3651 - val_acc: 0.5181\n",
      "Epoch 28/100\n",
      "500000/500000 [==============================] - 22s - loss: 1.3732 - acc: 0.4985 - val_loss: 1.3541 - val_acc: 0.5283\n",
      "Epoch 29/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.3516 - acc: 0.5062 - val_loss: 1.3189 - val_acc: 0.5265\n",
      "Epoch 30/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.3261 - acc: 0.5181 - val_loss: 1.3119 - val_acc: 0.5109\n",
      "Epoch 31/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.3197 - acc: 0.5162 - val_loss: 1.3156 - val_acc: 0.4821\n",
      "Epoch 32/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.2889 - acc: 0.5296 - val_loss: 1.2612 - val_acc: 0.5343\n",
      "Epoch 33/100\n",
      "500000/500000 [==============================] - 26s - loss: 1.2790 - acc: 0.5296 - val_loss: 1.2441 - val_acc: 0.5592\n",
      "Epoch 34/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.2813 - acc: 0.5297 - val_loss: 1.2760 - val_acc: 0.5601\n",
      "Epoch 35/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.2691 - acc: 0.5400 - val_loss: 1.2352 - val_acc: 0.5257\n",
      "Epoch 36/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.2536 - acc: 0.5450 - val_loss: 1.2971 - val_acc: 0.4912\n",
      "Epoch 37/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.2490 - acc: 0.5406 - val_loss: 1.2380 - val_acc: 0.5129\n",
      "Epoch 38/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.2137 - acc: 0.5557 - val_loss: 1.1841 - val_acc: 0.5740\n",
      "Epoch 39/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.1990 - acc: 0.5613 - val_loss: 1.1810 - val_acc: 0.5761\n",
      "Epoch 40/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.1847 - acc: 0.5672 - val_loss: 1.2246 - val_acc: 0.5427\n",
      "Epoch 41/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.1747 - acc: 0.5693 - val_loss: 1.1473 - val_acc: 0.6077\n",
      "Epoch 42/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.1654 - acc: 0.5692 - val_loss: 1.1175 - val_acc: 0.6126\n",
      "Epoch 43/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.1543 - acc: 0.5734 - val_loss: 1.1205 - val_acc: 0.5785\n",
      "Epoch 44/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.1354 - acc: 0.5848 - val_loss: 1.1190 - val_acc: 0.5732\n",
      "Epoch 45/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.1304 - acc: 0.5804 - val_loss: 1.0762 - val_acc: 0.6358\n",
      "Epoch 46/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.0895 - acc: 0.6010 - val_loss: 1.0790 - val_acc: 0.5794\n",
      "Epoch 47/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.0797 - acc: 0.6044 - val_loss: 1.0436 - val_acc: 0.6075\n",
      "Epoch 48/100\n",
      "500000/500000 [==============================] - 27s - loss: 1.0648 - acc: 0.6070 - val_loss: 1.0432 - val_acc: 0.6069\n",
      "Epoch 49/100\n",
      "500000/500000 [==============================] - 28s - loss: 1.0498 - acc: 0.6149 - val_loss: 1.0005 - val_acc: 0.6544\n",
      "Epoch 50/100\n",
      "500000/500000 [==============================] - 30s - loss: 1.0318 - acc: 0.6225 - val_loss: 1.0239 - val_acc: 0.6086\n",
      "Epoch 51/100\n",
      "500000/500000 [==============================] - 28s - loss: 1.0222 - acc: 0.6254 - val_loss: 0.9871 - val_acc: 0.6405\n",
      "Epoch 52/100\n",
      "500000/500000 [==============================] - 28s - loss: 1.0312 - acc: 0.6231 - val_loss: 0.9808 - val_acc: 0.6535\n",
      "Epoch 53/100\n",
      "500000/500000 [==============================] - 28s - loss: 0.9932 - acc: 0.6443 - val_loss: 1.0445 - val_acc: 0.5859\n",
      "Epoch 54/100\n",
      "500000/500000 [==============================] - 28s - loss: 0.9912 - acc: 0.6407 - val_loss: 0.9966 - val_acc: 0.6540\n",
      "Epoch 55/100\n",
      "500000/500000 [==============================] - 28s - loss: 0.9822 - acc: 0.6411 - val_loss: 0.9276 - val_acc: 0.7029\n",
      "Epoch 56/100\n",
      "500000/500000 [==============================] - 30s - loss: 0.9589 - acc: 0.6560 - val_loss: 0.9727 - val_acc: 0.6365\n",
      "Epoch 57/100\n",
      "500000/500000 [==============================] - 29s - loss: 0.9550 - acc: 0.6568 - val_loss: 0.9473 - val_acc: 0.6681\n",
      "Epoch 58/100\n",
      "500000/500000 [==============================] - 25s - loss: 0.9451 - acc: 0.6555 - val_loss: 0.9035 - val_acc: 0.6728\n",
      "Epoch 59/100\n",
      "500000/500000 [==============================] - 25s - loss: 0.9350 - acc: 0.6607 - val_loss: 0.9182 - val_acc: 0.6954\n",
      "Epoch 60/100\n",
      "500000/500000 [==============================] - 23s - loss: 0.9130 - acc: 0.6743 - val_loss: 0.9962 - val_acc: 0.6200\n",
      "Epoch 61/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.9130 - acc: 0.6736 - val_loss: 0.8821 - val_acc: 0.6995\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000/500000 [==============================] - 23s - loss: 0.9174 - acc: 0.6787 - val_loss: 0.8706 - val_acc: 0.7116\n",
      "Epoch 63/100\n",
      "500000/500000 [==============================] - 23s - loss: 0.8953 - acc: 0.6895 - val_loss: 0.8616 - val_acc: 0.7070\n",
      "Epoch 64/100\n",
      "500000/500000 [==============================] - 23s - loss: 0.8973 - acc: 0.6877 - val_loss: 0.8759 - val_acc: 0.7051\n",
      "Epoch 65/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8944 - acc: 0.6871 - val_loss: 0.9163 - val_acc: 0.6415\n",
      "Epoch 66/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8815 - acc: 0.6935 - val_loss: 0.8676 - val_acc: 0.6884\n",
      "Epoch 67/100\n",
      "500000/500000 [==============================] - 23s - loss: 0.8703 - acc: 0.7007 - val_loss: 1.0721 - val_acc: 0.6033\n",
      "Epoch 68/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8525 - acc: 0.7141 - val_loss: 0.8400 - val_acc: 0.7099\n",
      "Epoch 69/100\n",
      "500000/500000 [==============================] - 23s - loss: 0.8616 - acc: 0.7097 - val_loss: 0.8297 - val_acc: 0.7287\n",
      "Epoch 70/100\n",
      "500000/500000 [==============================] - 23s - loss: 0.8548 - acc: 0.7097 - val_loss: 0.8040 - val_acc: 0.7373\n",
      "Epoch 71/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8403 - acc: 0.7203 - val_loss: 0.8208 - val_acc: 0.7191\n",
      "Epoch 72/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8702 - acc: 0.7139 - val_loss: 0.8169 - val_acc: 0.7600\n",
      "Epoch 73/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8482 - acc: 0.7329 - val_loss: 0.8762 - val_acc: 0.7148\n",
      "Epoch 74/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8452 - acc: 0.7300 - val_loss: 0.7906 - val_acc: 0.7545\n",
      "Epoch 75/100\n",
      "500000/500000 [==============================] - 23s - loss: 0.8452 - acc: 0.7279 - val_loss: 0.8245 - val_acc: 0.7718\n",
      "Epoch 76/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8426 - acc: 0.7330 - val_loss: 0.8366 - val_acc: 0.7078\n",
      "Epoch 77/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8373 - acc: 0.7334 - val_loss: 0.8128 - val_acc: 0.7427\n",
      "Epoch 78/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8415 - acc: 0.7324 - val_loss: 0.7941 - val_acc: 0.7618\n",
      "Epoch 79/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8126 - acc: 0.7518 - val_loss: 0.7650 - val_acc: 0.7957\n",
      "Epoch 80/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8107 - acc: 0.7518 - val_loss: 0.8703 - val_acc: 0.6999\n",
      "Epoch 81/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8176 - acc: 0.7493 - val_loss: 0.8492 - val_acc: 0.7313\n",
      "Epoch 82/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.9133 - acc: 0.7159 - val_loss: 0.8807 - val_acc: 0.7356\n",
      "Epoch 83/100\n",
      "500000/500000 [==============================] - 23s - loss: 0.8352 - acc: 0.7880 - val_loss: 0.8643 - val_acc: 0.7586\n",
      "Epoch 84/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8697 - acc: 0.7618 - val_loss: 0.9428 - val_acc: 0.6924\n",
      "Epoch 85/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8696 - acc: 0.7660 - val_loss: 1.1427 - val_acc: 0.5927\n",
      "Epoch 86/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8676 - acc: 0.7555 - val_loss: 0.8315 - val_acc: 0.7493\n",
      "Epoch 87/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8737 - acc: 0.7534 - val_loss: 0.8355 - val_acc: 0.7933\n",
      "Epoch 88/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8179 - acc: 0.7725 - val_loss: 0.7336 - val_acc: 0.8127\n",
      "Epoch 89/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8158 - acc: 0.7685 - val_loss: 0.7601 - val_acc: 0.8228\n",
      "Epoch 90/100\n",
      "500000/500000 [==============================] - 23s - loss: 0.7774 - acc: 0.7903 - val_loss: 0.7549 - val_acc: 0.8019\n",
      "Epoch 91/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.7719 - acc: 0.7896 - val_loss: 0.8446 - val_acc: 0.7344\n",
      "Epoch 92/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.7814 - acc: 0.7827 - val_loss: 0.8390 - val_acc: 0.7638\n",
      "Epoch 93/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.7973 - acc: 0.7754 - val_loss: 0.7332 - val_acc: 0.8153\n",
      "Epoch 94/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.7343 - acc: 0.8166 - val_loss: 0.6844 - val_acc: 0.8611\n",
      "Epoch 95/100\n",
      "500000/500000 [==============================] - 24s - loss: 0.7642 - acc: 0.7916 - val_loss: 0.7580 - val_acc: 0.7883\n",
      "Epoch 96/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.7713 - acc: 0.7870 - val_loss: 0.6863 - val_acc: 0.8519\n",
      "Epoch 97/100\n",
      "500000/500000 [==============================] - 23s - loss: 0.7821 - acc: 0.8099 - val_loss: 0.7286 - val_acc: 0.8545\n",
      "Epoch 98/100\n",
      "500000/500000 [==============================] - 23s - loss: 0.7789 - acc: 0.8062 - val_loss: 0.7290 - val_acc: 0.8370\n",
      "Epoch 99/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.8037 - acc: 0.7998 - val_loss: 0.8615 - val_acc: 0.7620\n",
      "Epoch 100/100\n",
      "500000/500000 [==============================] - 22s - loss: 0.7820 - acc: 0.8138 - val_loss: 0.7440 - val_acc: 0.8435\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(xy_train, zz_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=training_epochs,\n",
    "                    verbose=1, # This is for what we want it to display out as it trains \n",
    "                    validation_data=(xy_test, zz_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_train(hist):\n",
    "    h = hist.history\n",
    "    if 'acc' in h:\n",
    "        meas='acc'\n",
    "        loc='lower right'\n",
    "    else:\n",
    "        meas='loss'\n",
    "        loc='upper right'\n",
    "    plt.plot(hist.history[meas])\n",
    "    plt.plot(hist.history['val_'+meas])\n",
    "    plt.title('model '+meas)\n",
    "    plt.ylabel(meas)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc=loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_train(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "#preds = model.predict(x_test[10:15])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
