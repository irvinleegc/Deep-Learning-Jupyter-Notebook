{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all necessary packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Function to clean text and build corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import regex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to clean the text\n",
    "def clean_text(text):\n",
    "    text = regex.sub(\"\\[http[^]]+? ([^]]+)]\", r\"\\1\", text) \n",
    "    text = regex.sub(\"\\[http[^]]+]\", \"\", text) \n",
    "    text = regex.sub(\"(?s)<ref>.+?</ref>\", \"\", text) # remove reference links\n",
    "    text = regex.sub(\"(?s)<[^>]+>\", \"\", text) # remove html tags\n",
    "    text = regex.sub(\"&[a-z]+;\", \"\", text) # remove html entities\n",
    "    text = regex.sub(\"(?s){{.+?}}\", \"\", text) # remove markup tags\n",
    "    text = regex.sub(\"(?s){.+?}\", \"\", text) # remove markup tags\n",
    "    text = regex.sub(\"(?s)\\[\\[([^]]+\\|)\", \"\", text) # remove link target strings\n",
    "    text = regex.sub(\"(?s)\\[\\[([^]]+\\:.+?]])\", \"\", text) # remove media links\n",
    "    \n",
    "    text = regex.sub(\"[']{5}\", \"\", text) # remove italic+bold symbols\n",
    "    text = regex.sub(\"[']{3}\", \"\", text) # remove bold symbols\n",
    "    text = regex.sub(\"[']{2}\", \"\", text) # remove italic symbols\n",
    "    \n",
    "    text = regex.sub(u\"[^ \\r\\n\\p{Latin}\\d\\-'.?!]\", \" \", text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = regex.sub(\"[ ]{2,}\", \" \", text) # Squeeze spaces.\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to build a corpus\n",
    "import glob\n",
    "import codecs\n",
    "\n",
    "def build_corpus():\n",
    "        \n",
    "    with codecs.open('data/en_wikinews.txt', 'w', 'utf-8') as fout:\n",
    "        fs = glob.glob('data/raw/*.xml')\n",
    "        ns = \"{http://www.mediawiki.org/xml/export-0.10/}\" # namespace\n",
    "        for f in fs:\n",
    "            i = 1\n",
    "            for _, elem in ET.iterparse(f, tag=ns+\"text\"):\n",
    "                try:\n",
    "                    if i > 5000:\n",
    "                        running_text = elem.text\n",
    "                        running_text = running_text.split(\"===\")[0]\n",
    "                        running_text = clean_text(running_text)\n",
    "                        paras = running_text.split(\"\\n\")\n",
    "                        for para in paras:\n",
    "                            if len(para) > 500:\n",
    "                                sents = [regex.sub(\"([.!?]+$)\", r\" \\1\", sent) for sent in sent_tokenize(para.strip())]\n",
    "                                fout.write(\" \".join(sents) + \"\\n\")\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                elem.clear() # We need to save memory!\n",
    "                i += 1\n",
    "                if i % 1000 == 0: print(i,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NLTK\n",
    "import nltk\n",
    "#SPACEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'glove'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b89595d58ac5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#word2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#GloVe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mglove\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'glove'"
     ]
    }
   ],
   "source": [
    "#word2vec\n",
    "#GloVe\n",
    "import glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first tokenized the corpus (tagging)\n",
    "# then train it with  a word embedding to get things into context\n",
    "\n",
    "#sentiment neuron.\n",
    "\n",
    "#doc2vc tutorial (rare technologies)\n",
    "# https://rare-technologies.com/doc2vec-tutorial/\n",
    "\n",
    "#skip thought\n",
    "# https://arxiv.org/abs/1506.06726\n",
    "# https://github.com/ryankiros/skip-thoughts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 read a corpus\n",
    "\n",
    "# 2 make look up table\n",
    "# for char RNN example: char to int / int to char\n",
    "# for word RNN example: word to int / int to word.. then pass the int to embedding / embedding to int then back to word (get the embedding of words then make the lookup table)\n",
    "\n",
    "# 3 create the data by chopping the corpus to pretict the next word / character.\n",
    "# eg: pass in 30 words, predict the next word. and pass the next word to calculate loss.\n",
    "# eg: pass in 30 character, predict the next character.  and pass the next word to calculate loss.\n",
    "\n",
    "# 4 normalized the input (divide by the number of class (character or words))\n",
    "\n",
    "\n",
    "# for chat bot, sometimes make a context vector that's based on the last five conversation replied to predict the next answer to the question.\n",
    "# example with time based, or personal preference.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
