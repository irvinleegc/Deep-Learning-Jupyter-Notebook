{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import all necessary packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "#callbacks to save after each epoch\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.8\n",
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Classes 40\n",
      "#Images per class 12000\n"
     ]
    }
   ],
   "source": [
    "# define the classes\n",
    "Classes= ['airplane','angel','apple','baseball_bat','brain','birthday_cake','bear','butterfly','canoe','cello',\n",
    "          'chair','chandelier','church','diamond','dishwasher','dragon','duck','elephant','fire_hydrant','flamingo',\n",
    "          'flower','giraffe','guitar','hand','headphones','hurricane','ice_cream','jail','lion','lipstick',\n",
    "          'map','moon','ocean','owl','pig','police_car','rabbit','rainbow','river','The_Mona_Lisa']\n",
    "\n",
    "IMAGE_PER_CLASS = 12000 #20 images per class\n",
    "NUM_CLASSES = len(Classes)\n",
    "\n",
    "print('#Classes', NUM_CLASSES )\n",
    "print('#Images per class', IMAGE_PER_CLASS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Train and Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File  MyData_40class_12000sample20.npy  saved\n",
      "File  MyData_40class_12000sample15000.npy  saved\n"
     ]
    }
   ],
   "source": [
    "# This cell is only if you are loading the data from scratch\n",
    "# generate data set\n",
    "def generate_data_set(classes, start_from, num_examples_per_class):\n",
    "    quickdraws = [np.load(\"./quickdraw/{}.npy\".format(qdraw))[start_from:(start_from+num_examples_per_class)] for qdraw in classes]\n",
    "    \n",
    "    # Concat the arrays together\n",
    "    x_data = np.concatenate(quickdraws,axis=0)\n",
    "    x_data.shape\n",
    "    \n",
    "    filename = 'MyData_'+str(NUM_CLASSES)+'class_'+str(num_examples_per_class)+'sample'+str(start_from)+'.npy'\n",
    "    np.save(filename,x_data)\n",
    "    print('File ', filename, ' saved')\n",
    "    return filename\n",
    "\n",
    "#Get the Data\n",
    "train_file = generate_data_set(Classes, 20, IMAGE_PER_CLASS)\n",
    "test_file = generate_data_set(Classes, 15000, IMAGE_PER_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480000, 784)\n",
      "(480000, 784)\n"
     ]
    }
   ],
   "source": [
    "#image_train_data = np.load(\"./MyData_40class_50sample1000.npy\")\n",
    "#image_test_data = np.load(\"./MyData_40class_50sample20.npy\")\n",
    "\n",
    "image_train_data = np.load(train_file)\n",
    "image_test_data = np.load(test_file)\n",
    "\n",
    "print(image_train_data.shape)\n",
    "print(image_test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480000,)\n",
      "(480000,)\n"
     ]
    }
   ],
   "source": [
    "## generate the labels for image\n",
    "labels = [np.full((IMAGE_PER_CLASS,), Classes.index(qdraw)) for qdraw in Classes]\n",
    "label_data = np.concatenate(labels,axis=0)\n",
    "\n",
    "label_train_data = label_data\n",
    "label_test_data = label_data\n",
    "\n",
    "print(label_train_data.shape)\n",
    "print(label_test_data.shape)\n",
    "#label_data.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_object(obj):\n",
    "    # Reshape 784 array into 28x28 image\n",
    "    image = obj.reshape([28,28])\n",
    "    fig, axes = plt.subplots(1, )\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    plt.imshow(image, cmap='gray_r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCD8QFOEHgvp/gwAvjUzo6csAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f73c2dd8d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane\n"
     ]
    }
   ],
   "source": [
    "show_object(image_train_data[25])\n",
    "print(Classes[label_train_data[25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_utils.to_categorical(label_train_data[25], NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shuffling function\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "image_train_data,label_train_data = unison_shuffled_copies(image_train_data,label_train_data)\n",
    "image_test_data,label_test_data = unison_shuffled_copies(image_test_data,label_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/EBQhB8IivADQRF+ICjCDwT1/6QkPG0osDSgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7395c28f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "police_car\n"
     ]
    }
   ],
   "source": [
    "show_object(image_train_data[25])\n",
    "print(Classes[label_train_data[25]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert label to one hot\n",
    "label_train_data = np_utils.to_categorical(label_train_data, NUM_CLASSES)\n",
    "label_test_data = np_utils.to_categorical(label_test_data, NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train_data[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape and normalized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_train_data = image_train_data.reshape(image_train_data.shape[0], 28, 28, 1)\n",
    "image_test_data = image_test_data.reshape(image_test_data.shape[0], 28, 28, 1)\n",
    "\n",
    "image_train_data = image_train_data.astype('float32')\n",
    "image_test_data = image_test_data.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the image\n",
    "image_train_data = image_train_data/255\n",
    "image_test_data = image_test_data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/EBQhB8IivADQRF+ICjCDwT1/6QkPG0osDSgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f73ac67a550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_object(image_train_data[25]) # should still show a (pale) image,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_img_gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "#                                   height_shift_range=0.08, zoom_range=0.08)\n",
    "\n",
    "train_img_gen = ImageDataGenerator()\n",
    "test_img_gen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_img_gen.flow(image_train_data, label_train_data, batch_size=BATCH_SIZE)\n",
    "test_generator = test_img_gen.flow(image_test_data, label_test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(features, labels, batch_size):\n",
    "    # Create empty arrays to contain batch of features and labels#\n",
    "    batch_features = np.zeros((batch_size, 64, 64, 3))\n",
    "    batch_labels = np.zeros((batch_size,1))\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            # choose random index in features\n",
    "            index= random.choice(len(features),1)\n",
    "            batch_features[i] = some_processing(features[index])\n",
    "            batch_labels[i] = labels[index]\n",
    "        yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the training paramaters\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "Conv2d_2 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "Maxpool2d_1 (MaxPooling2D)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv2d_3 (Conv2D)            (None, 14, 14, 16)        4624      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 16)        64        \n",
      "_________________________________________________________________\n",
      "Conv2d_4 (Conv2D)            (None, 14, 14, 8)         1160      \n",
      "_________________________________________________________________\n",
      "Maxpool2d_2 (MaxPooling2D)   (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 392)               0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 128)               50304     \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "Outputlayer (Dense)          (None, 40)                2600      \n",
      "=================================================================\n",
      "Total params: 76,704\n",
      "Trainable params: 76,608\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (28, 28, 1) #1 channel, 28x28 size\n",
    "\n",
    "inputs = Input(input_shape)\n",
    "x = Conv2D(filters=32, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_1\")(inputs)\n",
    "x = BatchNormalization(axis=-1)(x)\n",
    "x = Conv2D(filters=32, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_2\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='same', name = \"Maxpool2d_1\")(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x= Conv2D(filters=16, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_3\")(x)\n",
    "x = BatchNormalization(axis=-1)(x)\n",
    "x = Conv2D(filters=8, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_4\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='same', name = \"Maxpool2d_2\")(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='softmax', name = \"Dense_1\")(x)\n",
    "x = Dense(64, activation='softmax', name = \"Dense_2\")(x)\n",
    "outputs = Dense(NUM_CLASSES, activation='softmax', name = \"Outputlayer\")(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#Callback to save after each epoch\n",
    "checkpoint_filepath=\"./checkpoint-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, save_best_only=True)\n",
    "\n",
    "callbacks_list = [checkpointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7500/7500 [==============================] - 857s - loss: 2.7283 - acc: 0.2150 - val_loss: 2.2687 - val_acc: 0.2602\n",
      "Epoch 2/10\n",
      "7500/7500 [==============================] - 858s - loss: 2.0286 - acc: 0.3462 - val_loss: 1.7950 - val_acc: 0.4430\n",
      "Epoch 3/10\n",
      "7500/7500 [==============================] - 855s - loss: 1.5546 - acc: 0.5188 - val_loss: 1.3355 - val_acc: 0.6062\n",
      "Epoch 4/10\n",
      "7500/7500 [==============================] - 841s - loss: 1.2481 - acc: 0.6509 - val_loss: 1.0926 - val_acc: 0.7039\n",
      "Epoch 5/10\n",
      "7500/7500 [==============================] - 841s - loss: 1.0301 - acc: 0.7343 - val_loss: 0.9105 - val_acc: 0.7719\n",
      "Epoch 6/10\n",
      "7500/7500 [==============================] - 840s - loss: 0.8849 - acc: 0.7760 - val_loss: 0.8689 - val_acc: 0.7688\n",
      "Epoch 7/10\n",
      "7500/7500 [==============================] - 841s - loss: 0.8150 - acc: 0.7909 - val_loss: 0.9227 - val_acc: 0.7586\n",
      "Epoch 8/10\n",
      "7500/7500 [==============================] - 850s - loss: 0.7742 - acc: 0.7995 - val_loss: 0.7497 - val_acc: 0.8055\n",
      "Epoch 9/10\n",
      "7500/7500 [==============================] - 841s - loss: 0.7445 - acc: 0.8063 - val_loss: 0.7566 - val_acc: 0.7984\n",
      "Epoch 10/10\n",
      "7500/7500 [==============================] - 970s - loss: 0.7199 - acc: 0.8117 - val_loss: 0.7709 - val_acc: 0.7945\n"
     ]
    }
   ],
   "source": [
    "#compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#train the model\n",
    "hist = model.fit_generator(train_generator,\n",
    "                           epochs=EPOCHS,\n",
    "                           steps_per_epoch=(IMAGE_PER_CLASS*NUM_CLASSES/BATCH_SIZE),\n",
    "                           verbose=1,\n",
    "                           max_queue_size=20,\n",
    "                           validation_data=test_generator,\n",
    "                           validation_steps=20,\n",
    "                           callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gc/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `evaluate_generator` call to the Keras 2 API: `evaluate_generator(<keras.pre..., 10, 10, workers=1, use_multiprocessing=False)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy: 0.8234375\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate_generator(test_generator,10,10,workers=1,pickle_safe=False)\n",
    "print(\"model accuracy:\",metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('Quickdraw_Model1a.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and train further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model\n",
    "model = load_model('Quickdraw_Model1a.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "7499/7500 [============================>.] - ETA: 0s - loss: 0.7035 - acc: 0.8156Epoch 00000: val_loss improved from inf to 0.71894, saving model to ./checkpoint-00-0.82.hdf5\n",
      "7500/7500 [==============================] - 869s - loss: 0.7035 - acc: 0.8156 - val_loss: 0.7189 - val_acc: 0.8203\n",
      "Epoch 2/2\n",
      "7499/7500 [============================>.] - ETA: 0s - loss: 0.6900 - acc: 0.8183Epoch 00001: val_loss did not improve\n",
      "7500/7500 [==============================] - 866s - loss: 0.6900 - acc: 0.8183 - val_loss: 0.7726 - val_acc: 0.8039\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train the model\n",
    "hist = model.fit_generator(train_generator,\n",
    "                           epochs=2,\n",
    "                           steps_per_epoch=(IMAGE_PER_CLASS*NUM_CLASSES/BATCH_SIZE),\n",
    "                           verbose=1,\n",
    "                           max_queue_size=20,\n",
    "                           validation_data=test_generator,\n",
    "                           validation_steps=20,\n",
    "                           callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the model1a\n",
    "input_shape = (28, 28, 1) #1 channel, 28x28 size\n",
    "\n",
    "inputs = Input(input_shape) \n",
    "x = Conv2D(filters=32, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_1\")(inputs)\n",
    "x = BatchNormalization(axis=-1)(x)\n",
    "x = Conv2D(filters=16, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_2\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='same', name = \"Maxpool2d_1\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x= Conv2D(filters=16, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_3\")(x)\n",
    "x = BatchNormalization(axis=-1)(x)\n",
    "x = Conv2D(filters=8, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_4\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='same', name = \"Maxpool2d_2\")(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='softmax', name = \"Dense_1\")(x)\n",
    "x = Dense(128, activation='softmax', name = \"Dense_2\")(x)\n",
    "outputs = Dense(NUM_CLASSES, activation='softmax', name = \"Outputlayer\")(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#train the model\n",
    "hist = model.fit_generator(train_generator,\n",
    "                           epochs=EPOCHS,\n",
    "                           steps_per_epoch=(IMAGE_PER_CLASS*NUM_CLASSES/BATCH_SIZE),\n",
    "                           verbose=1,\n",
    "                           max_queue_size=20,\n",
    "                           validation_data=test_generator,\n",
    "                           validation_steps=20,\n",
    "                           callbacks=callbacks_list)\n",
    "\n",
    "model.save('Quickdraw_Model1b.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2\n",
    "Create a model using Sequencial method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 3 from 1 for 'conv2d_2/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    653\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 654\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    655\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_2/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-8b19a334609b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                 \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m             \u001b[1;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                 dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             outputs = K.conv3d(\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[0;32m   3162\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3163\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3164\u001b[1;33m         data_format='NHWC')\n\u001b[0m\u001b[0;32m   3165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_postprocess_conv2d_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution\u001b[1;34m(input, filter, padding, strides, dilation_rate, name, data_format)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mdilation_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m         op=op)\n\u001b[0m\u001b[0;32m    673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mwith_space_to_batch\u001b[1;34m(input, dilation_rate, padding, op, filter_shape, spatial_dims, data_format)\u001b[0m\n\u001b[0;32m    336\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dilation_rate must be positive\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconst_rate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_spatial_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m   \u001b[1;31m# We have two padding contributions. The first is used for converting \"SAME\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mop\u001b[1;34m(input_converted, _, padding)\u001b[0m\n\u001b[0;32m    662\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m           \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     return with_space_to_batch(\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m_non_atrous_convolution\u001b[1;34m(input, filter, padding, data_format, strides, name)\u001b[0m\n\u001b[0;32m    129\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m    132\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mconv_dims\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"NDHWC\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[0;32m    395\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[0;32m    398\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    768\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2630\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2631\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2632\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2633\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2634\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1909\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1861\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, require_shape_fn)\u001b[0m\n\u001b[0;32m    593\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    594\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m                                   require_shape_fn)\n\u001b[0m\u001b[0;32m    596\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    657\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_2/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32]."
     ]
    }
   ],
   "source": [
    "#define the model2 - sequencial\n",
    "input_shape = (28, 28, 1) #1 channel, 28x28 size\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#train the model\n",
    "hist = model.fit_generator(train_generator,\n",
    "                           epochs=EPOCHS,\n",
    "                           steps_per_epoch=(IMAGE_PER_CLASS*NUM_CLASSES/BATCH_SIZE),\n",
    "                           verbose=1,\n",
    "                           max_queue_size=20,\n",
    "                           validation_data=test_generator,\n",
    "                           validation_steps=20,\n",
    "                           callbacks=callbacks_list)\n",
    "\n",
    "model.save('Quickdraw_Model2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3\n",
    "try transfer learning to get VGG or ImageNet or Inception?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
