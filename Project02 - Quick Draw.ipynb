{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import all necessary packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import keras.backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.8\n",
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Classes 20\n",
      "#Image per class 50\n"
     ]
    }
   ],
   "source": [
    "# define the classes\n",
    "Classes= ['airplane','angel','apple','baseball_bat','birthday_cake','bear','cello','dragon','elephant','flamingo','lipstick','map','moon','ocean','owl','passport','pig','police_car','rainbow','The_Mona_Lisa']\n",
    "image_per_class = 50 #20 images per class\n",
    "print('#Classes', len(Classes))\n",
    "print('#Image per class', image_per_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is only if you are loading the data from scratch\n",
    "# generate data set\n",
    "def generate_data_set(classes, start_from, num_examples_per_class):\n",
    "    quickdraws = [np.load(\"./quickdraw/{}.npy\".format(qdraw))[start_from:(start_from+num_examples_per_class)] for qdraw in classes]\n",
    "    \n",
    "    # Concat the arrays together\n",
    "    x_data = np.concatenate(quickdraws,axis=0)\n",
    "    x_data.shape\n",
    "    \n",
    "    filename = str(len(classes))+'class_'+str(num_examples_per_class)+'sample'+str(start_from)+'.npy'\n",
    "    np.save('My_Data'+filename,x_data)\n",
    "\n",
    "#Get the Data\n",
    "generate_data_set(Classes, 20, image_per_class)\n",
    "generate_data_set(Classes, 1000, image_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 784)\n",
      "(1000, 784)\n"
     ]
    }
   ],
   "source": [
    "image_train_data = np.load(\"./My_Data20class_50sample1000.npy\")\n",
    "image_test_data = np.load(\"./My_Data20class_50sample20.npy\")\n",
    "\n",
    "print(image_train_data.shape)\n",
    "print(image_test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "## generate the labels for image\n",
    "labels = [np.full((image_per_class,), Classes.index(qdraw)) for qdraw in Classes]\n",
    "label_data = np.concatenate(labels,axis=0)\n",
    "label_train_data = label_data\n",
    "label_test_data = label_data\n",
    "\n",
    "print(label_train_data.shape)\n",
    "print(label_test_data.shape)\n",
    "#label_data.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_object(obj):\n",
    "    # Reshape 784 array into 28x28 image\n",
    "    image = obj.reshape([28,28])\n",
    "    fig, axes = plt.subplots(1, )\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    plt.imshow(image, cmap='gray_r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x975a7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_object(image_train_data[25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shuffling function\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "image_train_data,label_train_data = unison_shuffled_copies(image_train_data,label_train_data)\n",
    "image_test_data,label_test_data = unison_shuffled_copies(image_test_data,label_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9823a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_object(image_train_data[25])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(features, labels, batch_size):\n",
    "    # Create empty arrays to contain batch of features and labels#\n",
    "    batch_features = np.zeros((batch_size, 64, 64, 3))\n",
    "    batch_labels = np.zeros((batch_size,1))\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            # choose random index in features\n",
    "            index= random.choice(len(features),1)\n",
    "            batch_features[i] = some_processing(features[index])\n",
    "            batch_labels[i] = labels[index]\n",
    "        yield batch_features, batch_labels\n",
    "\n",
    "# Prepare images for training in batches\n",
    "batches = get_batches('./My_Data20class_50sample1000.npy', batch_size=batch_size)\n",
    "val_batches = get_batches('./My_Data20class_50sample20.npy', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the training paramaters\n",
    "batch_size = 100\n",
    "num_classes = 20\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1, 28, 28)         0         \n",
      "_________________________________________________________________\n",
      "Conv2d_1 (Conv2D)            (None, 1, 28, 32)         8096      \n",
      "_________________________________________________________________\n",
      "Conv2d_2 (Conv2D)            (None, 1, 28, 64)         18496     \n",
      "_________________________________________________________________\n",
      "Maxpool2d_1 (MaxPooling2D)   (None, 1, 14, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 128)               114816    \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "Outputlayer (Dense)          (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 160,500\n",
      "Trainable params: 160,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define the model1\n",
    "\n",
    "input_shape = (1, 28, 28) #1 channel, 28x28 size\n",
    "\n",
    "inputs = Input(input_shape) \n",
    "x = Conv2D(filters=32, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_1\")(inputs)\n",
    "x = Conv2D(filters=64, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_2\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='same', name = \"Maxpool2d_1\")(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='softmax', name = \"Dense_1\")(x)\n",
    "x = Dense(128, activation='softmax', name = \"Dense_2\")(x)\n",
    "outputs = Dense(len(Classes), activation='softmax', name = \"Outputlayer\")(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_batches() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-68e8cc71a0ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Prepare images for training in batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./My_Data20class_50sample1000.npy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mval_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./My_Data20class_50sample20.npy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_batches() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "#compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#train the model\n",
    "hist = model.fit_generator(batches,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          steps_per_epoch=5,\n",
    "          verbose=1,\n",
    "          max_queue_size=10,\n",
    "          validation_data=val_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = model.evaluate_generator(val_batches,10,10,workers=1,pickle_safe=False)\n",
    "print(\"model accuracy:\",metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('Quickdraw.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 3 from 1 for 'conv2d_2/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    653\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 654\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    655\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_2/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-8b19a334609b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                 \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m             \u001b[1;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                 dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             outputs = K.conv3d(\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[0;32m   3162\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3163\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3164\u001b[1;33m         data_format='NHWC')\n\u001b[0m\u001b[0;32m   3165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_postprocess_conv2d_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution\u001b[1;34m(input, filter, padding, strides, dilation_rate, name, data_format)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mdilation_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m         op=op)\n\u001b[0m\u001b[0;32m    673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mwith_space_to_batch\u001b[1;34m(input, dilation_rate, padding, op, filter_shape, spatial_dims, data_format)\u001b[0m\n\u001b[0;32m    336\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dilation_rate must be positive\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconst_rate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_spatial_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m   \u001b[1;31m# We have two padding contributions. The first is used for converting \"SAME\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mop\u001b[1;34m(input_converted, _, padding)\u001b[0m\n\u001b[0;32m    662\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m           \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     return with_space_to_batch(\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m_non_atrous_convolution\u001b[1;34m(input, filter, padding, data_format, strides, name)\u001b[0m\n\u001b[0;32m    129\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m    132\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mconv_dims\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"NDHWC\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[0;32m    395\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[0;32m    398\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    768\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2630\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2631\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2632\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2633\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2634\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1909\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1861\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, require_shape_fn)\u001b[0m\n\u001b[0;32m    593\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    594\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m                                   require_shape_fn)\n\u001b[0m\u001b[0;32m    596\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    657\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_2/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32]."
     ]
    }
   ],
   "source": [
    "#define the model2 - sequencial\n",
    "\n",
    "input_shape = (1, 28, 28) #1 channel, 28x28 size\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "model.fit(image_train_data, labal_train_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(image_test_data, label_test_data))\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#train the model\n",
    "model.fit(image_train_data, labal_train_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(image_test_data, label_test_data))\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_object(x_data[53000])\n",
    "print(y_data[53000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
