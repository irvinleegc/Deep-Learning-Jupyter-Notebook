{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import all necessary packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import keras.backend as K\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.8\n",
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Classes 40\n",
      "#Images per class 12000\n"
     ]
    }
   ],
   "source": [
    "# define the classes\n",
    "Classes= ['airplane','angel','apple','baseball_bat','brain','birthday_cake','bear','butterfly','canoe','cello',\n",
    "          'chair','chandelier','church','diamond','dishwasher','dragon','duck','elephant','fire_hydrant','flamingo',\n",
    "          'flower','giraffe','guitar','hand','headphones','hurricane','ice_cream','jail','lion','lipstick',\n",
    "          'map','moon','ocean','owl','pig','police_car','rabbit','rainbow','river','The_Mona_Lisa']\n",
    "\n",
    "IMAGE_PER_CLASS = 12000 #20 images per class\n",
    "NUM_CLASSES = len(Classes)\n",
    "\n",
    "print('#Classes', NUM_CLASSES )\n",
    "print('#Images per class', IMAGE_PER_CLASS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Train and Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File  MyData_40class_12000sample20.npy  saved\n",
      "File  MyData_40class_12000sample15000.npy  saved\n"
     ]
    }
   ],
   "source": [
    "# This cell is only if you are loading the data from scratch\n",
    "# generate data set\n",
    "def generate_data_set(classes, start_from, num_examples_per_class):\n",
    "    quickdraws = [np.load(\"./quickdraw/{}.npy\".format(qdraw))[start_from:(start_from+num_examples_per_class)] for qdraw in classes]\n",
    "    \n",
    "    # Concat the arrays together\n",
    "    x_data = np.concatenate(quickdraws,axis=0)\n",
    "    x_data.shape\n",
    "    \n",
    "    filename = 'MyData_'+str(NUM_CLASSES)+'class_'+str(num_examples_per_class)+'sample'+str(start_from)+'.npy'\n",
    "    np.save(filename,x_data)\n",
    "    print('File ', filename, ' saved')\n",
    "    return filename\n",
    "\n",
    "#Get the Data\n",
    "train_file = generate_data_set(Classes, 20, IMAGE_PER_CLASS)\n",
    "test_file = generate_data_set(Classes, 15000, IMAGE_PER_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480000, 784)\n",
      "(480000, 784)\n"
     ]
    }
   ],
   "source": [
    "#image_train_data = np.load(\"./MyData_40class_50sample1000.npy\")\n",
    "#image_test_data = np.load(\"./MyData_40class_50sample20.npy\")\n",
    "\n",
    "image_train_data = np.load(train_file)\n",
    "image_test_data = np.load(test_file)\n",
    "\n",
    "print(image_train_data.shape)\n",
    "print(image_test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480000,)\n",
      "(480000,)\n"
     ]
    }
   ],
   "source": [
    "## generate the labels for image\n",
    "labels = [np.full((IMAGE_PER_CLASS,), Classes.index(qdraw)) for qdraw in Classes]\n",
    "label_data = np.concatenate(labels,axis=0)\n",
    "\n",
    "label_train_data = label_data\n",
    "label_test_data = label_data\n",
    "\n",
    "print(label_train_data.shape)\n",
    "print(label_test_data.shape)\n",
    "#label_data.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_object(obj):\n",
    "    # Reshape 784 array into 28x28 image\n",
    "    image = obj.reshape([28,28])\n",
    "    fig, axes = plt.subplots(1, )\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    plt.imshow(image, cmap='gray_r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCD8QFOEHgvp/gwAvjUzo6csAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd3a7355c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane\n"
     ]
    }
   ],
   "source": [
    "show_object(image_train_data[25])\n",
    "print(Classes[label_train_data[25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_utils.to_categorical(label_train_data[25], NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shuffling function\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "image_train_data,label_train_data = unison_shuffled_copies(image_train_data,label_train_data)\n",
    "image_test_data,label_test_data = unison_shuffled_copies(image_test_data,label_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAD+1JREFUeJzt3X2MVGWWx/HfEREUMYoCIgP2iMZIjDqkIQbWlY2iDiI6\nMb6QOLDRLPPHEB1fUMI/QIKBKDqLEU1axWGSAZyoqDHEFfEFx2yUxpDRWVzGkHamF4RGfGFUmABn\n/+hi0qNdzy2rbtUtON9PQrr6nnqqTkp/favqufc+5u4CEM8xRTcAoBiEHwiK8ANBEX4gKMIPBEX4\ngaAIPxAU4QeCIvxAUMc28slOO+00b2lpaeRTAqF0dHRo9+7dVsl9awq/mV0laamkPpKedPfFqfu3\ntLSovb29lqcEkNDa2lrxfat+229mfSQtk/RTSaMlTTOz0dU+HoDGquUz/zhJH7v7Nnf/u6TVkq7N\npy0A9VZL+IdL+muP3ztL2/6Jmc00s3Yza+/q6qrh6QDkqZbw9/alwvfOD3b3NndvdffWwYMH1/B0\nAPJUS/g7JY3o8fuPJG2vrR0AjVJL+DdKOsfMfmxmx0m6WdJL+bQFoN6qnupz9wNmNkvSf6l7qm+5\nu/8pt84A1FVN8/zuvlbS2px6AdBAHN4LBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiB\noAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFANXaL7aLVv375k/ZVXXknW\nN2/enKwPGjQoWU+thDRixIiyNUkaO3Zsst6vX79kHUcu9vxAUIQfCIrwA0ERfiAowg8ERfiBoAg/\nEFRN8/xm1iFpr6SDkg64e2seTRVh48aNyfqyZcvK1l544YXk2C+//DJZP/bY9H+GAwcOJOu1GDhw\nYLJ+5ZVXJuszZsxI1idPnly2dswx7HuKlMdBPv/m7rtzeBwADcSfXiCoWsPvkl41s01mNjOPhgA0\nRq1v+ye4+3YzGyJpnZl95O4bet6h9EdhpiSNHDmyxqcDkJea9vzuvr30c5ekNZLG9XKfNndvdffW\n1AkoABqr6vCb2QAzG3j4tqQrJH2YV2MA6quWt/1DJa0xs8OPs9Ld0+euAmgaVYff3bdJujDHXupq\n6dKlyfrs2bOT9VNOOaVs7ZZbbkmOvemmm5L1CRMmJOvffPNNsr5z586ytW3btiXHrl27Nll//vnn\nk/Vnn302WT/rrLPK1ubNm5ccO3369GQdtWGqDwiK8ANBEX4gKMIPBEX4gaAIPxDUEXXp7tSprVnT\nQqtWrUrWb7755mT9iSeeKFs78cQTk2NrlfX4qfqoUaOSYydNmpSsP/TQQ8n6yy+/XPX4rNOBt27d\nmqwvXLgwWUcae34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCOqImudfv3592VrWPP6iRYuS9Tlz5lTV\n09Eu6/LaU6dOTdanTJlStnb77bcnx95///3Jeuqy4JI0fvz4ZD069vxAUIQfCIrwA0ERfiAowg8E\nRfiBoAg/ENQRNc//6quvlq1lnfN+9913590OKpA6TmDJkiXJsc8991yyfu+99ybrd9xxR9la//79\nk2PHjBmTrA8fPjxZPxKw5weCIvxAUIQfCIrwA0ERfiAowg8ERfiBoDLn+c1suaQpkna5+/mlbYMk\nPSOpRVKHpBvd/fP6tdnt9ddfL1u75JJLkmP79u2bdzuoUdZce2pZdEl65513aqqnDBgwIFlfsGBB\nsn7XXXcl62b2g3vKWyV7/t9Iuuo72+ZIWu/u50haX/odwBEkM/zuvkHSnu9svlbSitLtFZKuy7kv\nAHVW7Wf+oe6+Q5JKP4fk1xKARqj7F35mNtPM2s2svaurq95PB6BC1YZ/p5kNk6TSz13l7ujube7e\n6u6tgwcPrvLpAOSt2vC/JOnwEqszJL2YTzsAGiUz/Ga2StJ/SzrXzDrN7DZJiyVNMrM/S5pU+h3A\nESRznt/dp5UpXZZzL9q1q+ynB0nS5s2by9amT5+edzso2KFDh2oaP2PGjLK1665LT1CtXr06Wb/n\nnnuS9axjFG699dZkvRE4wg8IivADQRF+ICjCDwRF+IGgCD8QVFNdunvDhg1Vj73sstxnHlFnX3zx\nRbL+ySefJOtDhqRPKUld+vuZZ55Jjl24cGGy/tVXXyXrWVOB11xzTdlao46EZc8PBEX4gaAIPxAU\n4QeCIvxAUIQfCIrwA0E11Tz/p59+WvXYlpaW/BpBbr7++uuyteuvvz45NuuU3qzjQlKn1c6aNSs5\nNmuefs2aNcl66jLzkrRo0aKytYcffjg5Ni/s+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqKaa5//2\n22+rHtuvX78cO0Gl9u7dm6xfffXVZWvvvvtucuyqVauS9XPPPTdZT3n66aeT9TfeeCNZX7lyZbI+\nadKkZP2tt95K1huBPT8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBJU5z29myyVNkbTL3c8vbZsv6T8k\ndZXuNtfd19bazIUXXlj12Llz5ybrDz74YLJ+zDH8HezN559/nqyfffbZyXrq+vZZ58RPmTIlWa/F\ngAEDkvWsJbQfeeSRZP3OO+9M1tetW1e2duDAgeTYY4/N5/CcSv6P/42kq3rZ/mt3v6j0r+bgA2is\nzPC7+wZJexrQC4AGquW97iwz+6OZLTez8tdLAtCUqg3/45JGSbpI0g5JD5W7o5nNNLN2M2vv6uoq\ndzcADVZV+N19p7sfdPdDkp6QNC5x3zZ3b3X31kYtQAggW1XhN7NhPX79maQP82kHQKNUMtW3StJE\nSaeZWaekeZImmtlFklxSh6Rf1LFHAHWQGX53n9bL5qfq0IuuuOKKZD11LfUlS5Ykxz722GPJ+m23\n3Zasjxkzpmzt8ssvT44dOXJksl5PWd+zPP7448n60qVLk/Ws4wAuuOCCsrV6zuPX6uKLL07WH3jg\ngWR90KBByfr+/fvL1rZu3ZocO3r06GS9UhzZAgRF+IGgCD8QFOEHgiL8QFCEHwiqqS7dnSV1Wm5q\nSkmSpk+fnqwvW7asqp4k6bjjjkvWsy4x3bdv32Q9a6nqzs7OsrXdu3cnx2a54YYbkvU+ffok62++\n+WZNz1+U1tbWmsY/+eSTVY997bXXknWm+gDUhPADQRF+ICjCDwRF+IGgCD8QFOEHgjqi5vlTpk6d\nWtP42bNnJ+upU4Kz5l1HjRqVrB88eDBZz3LppZeWrWUte97W1pasL1iwIFlfv359sr569eqytT17\n0teFzTottp5GjBiRrN93333J+qZNm5L1jo6OsrXU5c7zxJ4fCIrwA0ERfiAowg8ERfiBoAg/EBTh\nB4I6aub59+3bV9P4rHPuTzrppLK1cePKLlgkSXr00Uer6ikPb7/9drKeNc//2WefJevnnXfeD+7p\nsI8++ihZHz9+fNWPXW+LFy8uuoWasecHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAy5/nNbISk30o6\nXdIhSW3uvtTMBkl6RlKLpA5JN7p7er3mOso6bz3L8ccfX3W91ueup1NPPbWm8Vnz/GPHjq36sbds\n2ZKsN/M8/9Ggkj3/AUl3u/t5ki6W9EszGy1pjqT17n6OpPWl3wEcITLD7+473P390u29krZIGi7p\nWkkrSndbIem6ejUJIH8/6DO/mbVI+omkdyUNdfcdUvcfCElD8m4OQP1UHH4zO1HSc5J+5e4VX2TM\nzGaaWbuZtXd1dVXTI4A6qCj8ZtZX3cH/nbs/X9q808yGlerDJO3qbay7t7l7q7u3Dh48OI+eAeQg\nM/xmZpKekrTF3R/uUXpJ0ozS7RmSXsy/PQD1UskpvRMk/VzSB2a2ubRtrqTFkn5vZrdJ+ouk9FrO\ndVbrKb39+/dP1pnq690ZZ5yRrA8ZUv6roO3bt1fVE/KRGX53/4MkK1O+LN92ADQKR/gBQRF+ICjC\nDwRF+IGgCD8QFOEHgjpqLt198sknJ+vDhw9P1rOW0T7hhBPK1vbv358cW6ShQ4cm6/Pnz0/WJ06c\nWNPzv/fee2VrHPFZLPb8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxDUUTPPf/rppyfrnZ2dNT1+aj58\n4MCBNT12kebNm1fXxz/zzDPr+vioHnt+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwjqqJnnr7fJkycX\n3QKQK/b8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBUZvjNbISZvWFmW8zsT2Z2R2n7fDP7PzPbXPrH\nRDhwBKnkIJ8Dku529/fNbKCkTWa2rlT7tbsvqV97AOolM/zuvkPSjtLtvWa2RVJ6+RsATe8HfeY3\nsxZJP5H0bmnTLDP7o5ktN7NTyoyZaWbtZtbe1dVVU7MA8lNx+M3sREnPSfqVu38l6XFJoyRdpO53\nBg/1Ns7d29y91d1bWZsNaB4Vhd/M+qo7+L9z9+clyd13uvtBdz8k6QlJ4+rXJoC8VfJtv0l6StIW\nd3+4x/ZhPe72M0kf5t8egHqp5Nv+CZJ+LukDM9tc2jZX0jQzu0iSS+qQ9Iu6dAigLir5tv8PkqyX\n0tr82wHQKBzhBwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQf\nCMrcvXFPZtYl6ZMem06TtLthDfwwzdpbs/Yl0Vu18uztTHev6Hp5DQ3/957crN3dWwtrIKFZe2vW\nviR6q1ZRvfG2HwiK8ANBFR3+toKfP6VZe2vWviR6q1YhvRX6mR9AcYre8wMoSCHhN7OrzOx/zexj\nM5tTRA/lmFmHmX1QWnm4veBelpvZLjP7sMe2QWa2zsz+XPrZ6zJpBfXWFCs3J1aWLvS1a7YVrxv+\ntt/M+kjaKmmSpE5JGyVNc/f/aWgjZZhZh6RWdy98TtjM/lXS3yT91t3PL217QNIed19c+sN5irvf\n1yS9zZf0t6JXbi4tKDOs58rSkq6T9O8q8LVL9HWjCnjditjzj5P0sbtvc/e/S1ot6doC+mh67r5B\n0p7vbL5W0orS7RXq/p+n4cr01hTcfYe7v1+6vVfS4ZWlC33tEn0VoojwD5f01x6/d6q5lvx2Sa+a\n2SYzm1l0M70YWlo2/fDy6UMK7ue7MldubqTvrCzdNK9dNSte562I8Pe2+k8zTTlMcPcxkn4q6Zel\nt7eoTEUrNzdKLytLN4VqV7zOWxHh75Q0osfvP5K0vYA+euXu20s/d0lao+ZbfXjn4UVSSz93FdzP\nPzTTys29rSytJnjtmmnF6yLCv1HSOWb2YzM7TtLNkl4qoI/vMbMBpS9iZGYDJF2h5lt9+CVJM0q3\nZ0h6scBe/kmzrNxcbmVpFfzaNduK14Uc5FOayvhPSX0kLXf3+xveRC/M7Cx17+2l7kVMVxbZm5mt\nkjRR3Wd97ZQ0T9ILkn4vaaSkv0i6wd0b/sVbmd4mqvut6z9Wbj78GbvBvf2LpLclfSDpUGnzXHV/\nvi7stUv0NU0FvG4c4QcExRF+QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeC+n9WqG2Sc3fMgwAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd37a1a5b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elephant\n"
     ]
    }
   ],
   "source": [
    "show_object(image_train_data[25])\n",
    "print(Classes[label_train_data[25]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert label to one hot\n",
    "label_train_data = np_utils.to_categorical(label_train_data, NUM_CLASSES)\n",
    "label_test_data = np_utils.to_categorical(label_test_data, NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train_data[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape and normalized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train_data = image_train_data.reshape(image_train_data.shape[0], 28, 28, 1)\n",
    "image_test_data = image_test_data.reshape(image_test_data.shape[0], 28, 28, 1)\n",
    "\n",
    "image_train_data = image_train_data.astype('float32')\n",
    "image_test_data = image_test_data.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the image\n",
    "image_train_data = image_train_data/255\n",
    "image_test_data = image_test_data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAD+1JREFUeJzt3X2MVGWWx/HfEREUMYoCIgP2iMZIjDqkIQbWlY2iDiI6\nMb6QOLDRLPPHEB1fUMI/QIKBKDqLEU1axWGSAZyoqDHEFfEFx2yUxpDRWVzGkHamF4RGfGFUmABn\n/+hi0qNdzy2rbtUtON9PQrr6nnqqTkp/favqufc+5u4CEM8xRTcAoBiEHwiK8ANBEX4gKMIPBEX4\ngaAIPxAU4QeCIvxAUMc28slOO+00b2lpaeRTAqF0dHRo9+7dVsl9awq/mV0laamkPpKedPfFqfu3\ntLSovb29lqcEkNDa2lrxfat+229mfSQtk/RTSaMlTTOz0dU+HoDGquUz/zhJH7v7Nnf/u6TVkq7N\npy0A9VZL+IdL+muP3ztL2/6Jmc00s3Yza+/q6qrh6QDkqZbw9/alwvfOD3b3NndvdffWwYMH1/B0\nAPJUS/g7JY3o8fuPJG2vrR0AjVJL+DdKOsfMfmxmx0m6WdJL+bQFoN6qnupz9wNmNkvSf6l7qm+5\nu/8pt84A1FVN8/zuvlbS2px6AdBAHN4LBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiB\noAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFANXaL7aLVv375k/ZVXXknW\nN2/enKwPGjQoWU+thDRixIiyNUkaO3Zsst6vX79kHUcu9vxAUIQfCIrwA0ERfiAowg8ERfiBoAg/\nEFRN8/xm1iFpr6SDkg64e2seTRVh48aNyfqyZcvK1l544YXk2C+//DJZP/bY9H+GAwcOJOu1GDhw\nYLJ+5ZVXJuszZsxI1idPnly2dswx7HuKlMdBPv/m7rtzeBwADcSfXiCoWsPvkl41s01mNjOPhgA0\nRq1v+ye4+3YzGyJpnZl95O4bet6h9EdhpiSNHDmyxqcDkJea9vzuvr30c5ekNZLG9XKfNndvdffW\n1AkoABqr6vCb2QAzG3j4tqQrJH2YV2MA6quWt/1DJa0xs8OPs9Ld0+euAmgaVYff3bdJujDHXupq\n6dKlyfrs2bOT9VNOOaVs7ZZbbkmOvemmm5L1CRMmJOvffPNNsr5z586ytW3btiXHrl27Nll//vnn\nk/Vnn302WT/rrLPK1ubNm5ccO3369GQdtWGqDwiK8ANBEX4gKMIPBEX4gaAIPxDUEXXp7tSprVnT\nQqtWrUrWb7755mT9iSeeKFs78cQTk2NrlfX4qfqoUaOSYydNmpSsP/TQQ8n6yy+/XPX4rNOBt27d\nmqwvXLgwWUcae34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCOqImudfv3592VrWPP6iRYuS9Tlz5lTV\n09Eu6/LaU6dOTdanTJlStnb77bcnx95///3Jeuqy4JI0fvz4ZD069vxAUIQfCIrwA0ERfiAowg8E\nRfiBoAg/ENQRNc//6quvlq1lnfN+9913590OKpA6TmDJkiXJsc8991yyfu+99ybrd9xxR9la//79\nk2PHjBmTrA8fPjxZPxKw5weCIvxAUIQfCIrwA0ERfiAowg8ERfiBoDLn+c1suaQpkna5+/mlbYMk\nPSOpRVKHpBvd/fP6tdnt9ddfL1u75JJLkmP79u2bdzuoUdZce2pZdEl65513aqqnDBgwIFlfsGBB\nsn7XXXcl62b2g3vKWyV7/t9Iuuo72+ZIWu/u50haX/odwBEkM/zuvkHSnu9svlbSitLtFZKuy7kv\nAHVW7Wf+oe6+Q5JKP4fk1xKARqj7F35mNtPM2s2svaurq95PB6BC1YZ/p5kNk6TSz13l7ujube7e\n6u6tgwcPrvLpAOSt2vC/JOnwEqszJL2YTzsAGiUz/Ga2StJ/SzrXzDrN7DZJiyVNMrM/S5pU+h3A\nESRznt/dp5UpXZZzL9q1q+ynB0nS5s2by9amT5+edzso2KFDh2oaP2PGjLK1665LT1CtXr06Wb/n\nnnuS9axjFG699dZkvRE4wg8IivADQRF+ICjCDwRF+IGgCD8QVFNdunvDhg1Vj73sstxnHlFnX3zx\nRbL+ySefJOtDhqRPKUld+vuZZ55Jjl24cGGy/tVXXyXrWVOB11xzTdlao46EZc8PBEX4gaAIPxAU\n4QeCIvxAUIQfCIrwA0E11Tz/p59+WvXYlpaW/BpBbr7++uuyteuvvz45NuuU3qzjQlKn1c6aNSs5\nNmuefs2aNcl66jLzkrRo0aKytYcffjg5Ni/s+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqKaa5//2\n22+rHtuvX78cO0Gl9u7dm6xfffXVZWvvvvtucuyqVauS9XPPPTdZT3n66aeT9TfeeCNZX7lyZbI+\nadKkZP2tt95K1huBPT8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBJU5z29myyVNkbTL3c8vbZsv6T8k\ndZXuNtfd19bazIUXXlj12Llz5ybrDz74YLJ+zDH8HezN559/nqyfffbZyXrq+vZZ58RPmTIlWa/F\ngAEDkvWsJbQfeeSRZP3OO+9M1tetW1e2duDAgeTYY4/N5/CcSv6P/42kq3rZ/mt3v6j0r+bgA2is\nzPC7+wZJexrQC4AGquW97iwz+6OZLTez8tdLAtCUqg3/45JGSbpI0g5JD5W7o5nNNLN2M2vv6uoq\ndzcADVZV+N19p7sfdPdDkp6QNC5x3zZ3b3X31kYtQAggW1XhN7NhPX79maQP82kHQKNUMtW3StJE\nSaeZWaekeZImmtlFklxSh6Rf1LFHAHWQGX53n9bL5qfq0IuuuOKKZD11LfUlS5Ykxz722GPJ+m23\n3Zasjxkzpmzt8ssvT44dOXJksl5PWd+zPP7448n60qVLk/Ws4wAuuOCCsrV6zuPX6uKLL07WH3jg\ngWR90KBByfr+/fvL1rZu3ZocO3r06GS9UhzZAgRF+IGgCD8QFOEHgiL8QFCEHwiqqS7dnSV1Wm5q\nSkmSpk+fnqwvW7asqp4k6bjjjkvWsy4x3bdv32Q9a6nqzs7OsrXdu3cnx2a54YYbkvU+ffok62++\n+WZNz1+U1tbWmsY/+eSTVY997bXXknWm+gDUhPADQRF+ICjCDwRF+IGgCD8QFOEHgjqi5vlTpk6d\nWtP42bNnJ+upU4Kz5l1HjRqVrB88eDBZz3LppZeWrWUte97W1pasL1iwIFlfv359sr569eqytT17\n0teFzTottp5GjBiRrN93333J+qZNm5L1jo6OsrXU5c7zxJ4fCIrwA0ERfiAowg8ERfiBoAg/EBTh\nB4I6aub59+3bV9P4rHPuTzrppLK1cePKLlgkSXr00Uer6ikPb7/9drKeNc//2WefJevnnXfeD+7p\nsI8++ihZHz9+fNWPXW+LFy8uuoWasecHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAy5/nNbISk30o6\nXdIhSW3uvtTMBkl6RlKLpA5JN7p7er3mOso6bz3L8ccfX3W91ueup1NPPbWm8Vnz/GPHjq36sbds\n2ZKsN/M8/9Ggkj3/AUl3u/t5ki6W9EszGy1pjqT17n6OpPWl3wEcITLD7+473P390u29krZIGi7p\nWkkrSndbIem6ejUJIH8/6DO/mbVI+omkdyUNdfcdUvcfCElD8m4OQP1UHH4zO1HSc5J+5e4VX2TM\nzGaaWbuZtXd1dVXTI4A6qCj8ZtZX3cH/nbs/X9q808yGlerDJO3qbay7t7l7q7u3Dh48OI+eAeQg\nM/xmZpKekrTF3R/uUXpJ0ozS7RmSXsy/PQD1UskpvRMk/VzSB2a2ubRtrqTFkn5vZrdJ+ouk9FrO\ndVbrKb39+/dP1pnq690ZZ5yRrA8ZUv6roO3bt1fVE/KRGX53/4MkK1O+LN92ADQKR/gBQRF+ICjC\nDwRF+IGgCD8QFOEHgjpqLt198sknJ+vDhw9P1rOW0T7hhBPK1vbv358cW6ShQ4cm6/Pnz0/WJ06c\nWNPzv/fee2VrHPFZLPb8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxDUUTPPf/rppyfrnZ2dNT1+aj58\n4MCBNT12kebNm1fXxz/zzDPr+vioHnt+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwjqqJnnr7fJkycX\n3QKQK/b8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBUZvjNbISZvWFmW8zsT2Z2R2n7fDP7PzPbXPrH\nRDhwBKnkIJ8Dku529/fNbKCkTWa2rlT7tbsvqV97AOolM/zuvkPSjtLtvWa2RVJ6+RsATe8HfeY3\nsxZJP5H0bmnTLDP7o5ktN7NTyoyZaWbtZtbe1dVVU7MA8lNx+M3sREnPSfqVu38l6XFJoyRdpO53\nBg/1Ns7d29y91d1bWZsNaB4Vhd/M+qo7+L9z9+clyd13uvtBdz8k6QlJ4+rXJoC8VfJtv0l6StIW\nd3+4x/ZhPe72M0kf5t8egHqp5Nv+CZJ+LukDM9tc2jZX0jQzu0iSS+qQ9Iu6dAigLir5tv8PkqyX\n0tr82wHQKBzhBwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQf\nCMrcvXFPZtYl6ZMem06TtLthDfwwzdpbs/Yl0Vu18uztTHev6Hp5DQ3/957crN3dWwtrIKFZe2vW\nviR6q1ZRvfG2HwiK8ANBFR3+toKfP6VZe2vWviR6q1YhvRX6mR9AcYre8wMoSCHhN7OrzOx/zexj\nM5tTRA/lmFmHmX1QWnm4veBelpvZLjP7sMe2QWa2zsz+XPrZ6zJpBfXWFCs3J1aWLvS1a7YVrxv+\ntt/M+kjaKmmSpE5JGyVNc/f/aWgjZZhZh6RWdy98TtjM/lXS3yT91t3PL217QNIed19c+sN5irvf\n1yS9zZf0t6JXbi4tKDOs58rSkq6T9O8q8LVL9HWjCnjditjzj5P0sbtvc/e/S1ot6doC+mh67r5B\n0p7vbL5W0orS7RXq/p+n4cr01hTcfYe7v1+6vVfS4ZWlC33tEn0VoojwD5f01x6/d6q5lvx2Sa+a\n2SYzm1l0M70YWlo2/fDy6UMK7ue7MldubqTvrCzdNK9dNSte562I8Pe2+k8zTTlMcPcxkn4q6Zel\nt7eoTEUrNzdKLytLN4VqV7zOWxHh75Q0osfvP5K0vYA+euXu20s/d0lao+ZbfXjn4UVSSz93FdzP\nPzTTys29rSytJnjtmmnF6yLCv1HSOWb2YzM7TtLNkl4qoI/vMbMBpS9iZGYDJF2h5lt9+CVJM0q3\nZ0h6scBe/kmzrNxcbmVpFfzaNduK14Uc5FOayvhPSX0kLXf3+xveRC/M7Cx17+2l7kVMVxbZm5mt\nkjRR3Wd97ZQ0T9ILkn4vaaSkv0i6wd0b/sVbmd4mqvut6z9Wbj78GbvBvf2LpLclfSDpUGnzXHV/\nvi7stUv0NU0FvG4c4QcExRF+QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeC+n9WqG2Sc3fMgwAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd390c140b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_object(image_train_data[25]) # should still show a (pale) image,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_img_gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "#                                   height_shift_range=0.08, zoom_range=0.08)\n",
    "\n",
    "train_img_gen = ImageDataGenerator()\n",
    "test_img_gen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_img_gen.flow(image_train_data, label_train_data, batch_size=BATCH_SIZE)\n",
    "test_generator = test_img_gen.flow(image_test_data, label_test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(features, labels, batch_size):\n",
    "    # Create empty arrays to contain batch of features and labels#\n",
    "    batch_features = np.zeros((batch_size, 64, 64, 3))\n",
    "    batch_labels = np.zeros((batch_size,1))\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            # choose random index in features\n",
    "            index= random.choice(len(features),1)\n",
    "            batch_features[i] = some_processing(features[index])\n",
    "            batch_labels[i] = labels[index]\n",
    "        yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the training paramaters\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "Conv2d_2 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "Maxpool2d_1 (MaxPooling2D)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv2d_3 (Conv2D)            (None, 14, 14, 16)        4624      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 16)        64        \n",
      "_________________________________________________________________\n",
      "Conv2d_4 (Conv2D)            (None, 14, 14, 8)         1160      \n",
      "_________________________________________________________________\n",
      "Maxpool2d_2 (MaxPooling2D)   (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 392)               0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 128)               50304     \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "Outputlayer (Dense)          (None, 40)                5160      \n",
      "=================================================================\n",
      "Total params: 87,520\n",
      "Trainable params: 87,424\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define the model1\n",
    "input_shape = (28, 28, 1) #1 channel, 28x28 size\n",
    "\n",
    "inputs = Input(input_shape) \n",
    "x = Conv2D(filters=32, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_1\")(inputs)\n",
    "x = BatchNormalization(axis=-1)(x)\n",
    "x = Conv2D(filters=32, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_2\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='same', name = \"Maxpool2d_1\")(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x= Conv2D(filters=16, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_3\")(x)\n",
    "x = BatchNormalization(axis=-1)(x)\n",
    "x = Conv2D(filters=8, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_4\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='same', name = \"Maxpool2d_2\")(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='softmax', name = \"Dense_1\")(x)\n",
    "x = Dense(128, activation='softmax', name = \"Dense_2\")(x)\n",
    "outputs = Dense(NUM_CLASSES, activation='softmax', name = \"Outputlayer\")(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7500/7500 [==============================] - 894s - loss: 2.8518 - acc: 0.1734 - val_loss: 2.2844 - val_acc: 0.2906\n",
      "Epoch 2/10\n",
      "7500/7500 [==============================] - 987s - loss: 1.9755 - acc: 0.3749 - val_loss: 1.5993 - val_acc: 0.5141\n",
      "Epoch 3/10\n",
      "7500/7500 [==============================] - 904s - loss: 1.4753 - acc: 0.5472 - val_loss: 1.3099 - val_acc: 0.6156\n",
      "Epoch 4/10\n",
      "7500/7500 [==============================] - 864s - loss: 1.1769 - acc: 0.6580 - val_loss: 1.0457 - val_acc: 0.6969\n",
      "Epoch 5/10\n",
      "7500/7500 [==============================] - 866s - loss: 0.9816 - acc: 0.7347 - val_loss: 0.9115 - val_acc: 0.7531\n",
      "Epoch 6/10\n",
      "7500/7500 [==============================] - 871s - loss: 0.8400 - acc: 0.7825 - val_loss: 0.7808 - val_acc: 0.8000\n",
      "Epoch 7/10\n",
      "7500/7500 [==============================] - 872s - loss: 0.7731 - acc: 0.7965 - val_loss: 0.7427 - val_acc: 0.8063\n",
      "Epoch 8/10\n",
      "2945/7500 [==========>...................] - ETA: 536s - loss: 0.7474 - acc: 0.8042"
     ]
    }
   ],
   "source": [
    "#compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#train the model\n",
    "hist = model.fit_generator(train_generator,\n",
    "                           epochs=EPOCHS,\n",
    "                           steps_per_epoch=(IMAGE_PER_CLASS*NUM_CLASSES/BATCH_SIZE),\n",
    "                           verbose=1,\n",
    "                           max_queue_size=20,\n",
    "                           validation_data=test_generator,\n",
    "                           validation_steps=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = model.evaluate_generator(test_generator,10,10,workers=1,pickle_safe=False)\n",
    "print(\"model accuracy:\",metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('Quickdraw_Model1a.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the model1a\n",
    "input_shape = (28, 28, 1) #1 channel, 28x28 size\n",
    "\n",
    "inputs = Input(input_shape) \n",
    "x = Conv2D(filters=32, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_1\")(inputs)\n",
    "x = BatchNormalization(axis=-1)(x)\n",
    "x = Conv2D(filters=16, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_2\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='same', name = \"Maxpool2d_1\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x= Conv2D(filters=16, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_3\")(x)\n",
    "x = BatchNormalization(axis=-1)(x)\n",
    "x = Conv2D(filters=8, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_4\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='same', name = \"Maxpool2d_2\")(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='softmax', name = \"Dense_1\")(x)\n",
    "x = Dense(128, activation='softmax', name = \"Dense_2\")(x)\n",
    "outputs = Dense(NUM_CLASSES, activation='softmax', name = \"Outputlayer\")(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#train the model\n",
    "hist = model.fit_generator(train_generator,\n",
    "                           epochs=EPOCHS,\n",
    "                           steps_per_epoch=(IMAGE_PER_CLASS*NUM_CLASSES/BATCH_SIZE),\n",
    "                           verbose=1,\n",
    "                           max_queue_size=10,\n",
    "                           validation_data=test_generator,\n",
    "                           validation_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2\n",
    "Create a model using Sequencial method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 3 from 1 for 'conv2d_2/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    653\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 654\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    655\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_2/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-8b19a334609b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                 \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m             \u001b[1;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                 dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             outputs = K.conv3d(\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[0;32m   3162\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3163\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3164\u001b[1;33m         data_format='NHWC')\n\u001b[0m\u001b[0;32m   3165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_postprocess_conv2d_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution\u001b[1;34m(input, filter, padding, strides, dilation_rate, name, data_format)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mdilation_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m         op=op)\n\u001b[0m\u001b[0;32m    673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mwith_space_to_batch\u001b[1;34m(input, dilation_rate, padding, op, filter_shape, spatial_dims, data_format)\u001b[0m\n\u001b[0;32m    336\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dilation_rate must be positive\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconst_rate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_spatial_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m   \u001b[1;31m# We have two padding contributions. The first is used for converting \"SAME\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mop\u001b[1;34m(input_converted, _, padding)\u001b[0m\n\u001b[0;32m    662\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m           \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     return with_space_to_batch(\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m_non_atrous_convolution\u001b[1;34m(input, filter, padding, data_format, strides, name)\u001b[0m\n\u001b[0;32m    129\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m    132\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mconv_dims\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"NDHWC\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[0;32m    395\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[0;32m    398\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    768\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2630\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2631\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2632\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2633\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2634\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1909\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1861\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, require_shape_fn)\u001b[0m\n\u001b[0;32m    593\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    594\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m                                   require_shape_fn)\n\u001b[0m\u001b[0;32m    596\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    657\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_2/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32]."
     ]
    }
   ],
   "source": [
    "#define the model2 - sequencial\n",
    "input_shape = (28, 28, 1) #1 channel, 28x28 size\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "model.fit(image_train_data, labal_train_data,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          validation_data=(image_test_data, label_test_data))\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3\n",
    "try transfer learning to get VGG or ImageNet or Inception?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
