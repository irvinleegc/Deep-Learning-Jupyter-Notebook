{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import all necessary packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import keras.backend as K\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.8\n",
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Classes 40\n",
      "#Images per class 10000\n"
     ]
    }
   ],
   "source": [
    "# define the classes\n",
    "Classes= ['airplane','angel','apple','baseball_bat','brain','birthday_cake','bear','butterfly','canoe','cello',\n",
    "          'chair','chandelier','church','diamond','dishwasher','dragon','duck','elephant','fire_hydrant','flamingo',\n",
    "          'flower','giraffe','guitar','hand','headphones','hurricane','ice_cream','jail','lion','lipstick',\n",
    "          'map','moon','ocean','owl','pig','police_car','rabbit','rainbow','river','The_Mona_Lisa']\n",
    "\n",
    "IMAGE_PER_CLASS = 10000 #20 images per class\n",
    "NUM_CLASSES = len(Classes)\n",
    "\n",
    "print('#Classes', NUM_CLASSES )\n",
    "print('#Images per class', IMAGE_PER_CLASS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Train and Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File  MyData_40class_10000sample20.npy  saved\n",
      "File  MyData_40class_10000sample300.npy  saved\n"
     ]
    }
   ],
   "source": [
    "# This cell is only if you are loading the data from scratch\n",
    "# generate data set\n",
    "def generate_data_set(classes, start_from, num_examples_per_class):\n",
    "    quickdraws = [np.load(\"./quickdraw/{}.npy\".format(qdraw))[start_from:(start_from+num_examples_per_class)] for qdraw in classes]\n",
    "    \n",
    "    # Concat the arrays together\n",
    "    x_data = np.concatenate(quickdraws,axis=0)\n",
    "    x_data.shape\n",
    "    \n",
    "    filename = 'MyData_'+str(NUM_CLASSES)+'class_'+str(num_examples_per_class)+'sample'+str(start_from)+'.npy'\n",
    "    np.save(filename,x_data)\n",
    "    print('File ', filename, ' saved')\n",
    "    return filename\n",
    "\n",
    "#Get the Data\n",
    "train_file = generate_data_set(Classes, 20, IMAGE_PER_CLASS)\n",
    "test_file = generate_data_set(Classes, 15000, IMAGE_PER_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 784)\n",
      "(400000, 784)\n"
     ]
    }
   ],
   "source": [
    "#image_train_data = np.load(\"./MyData_40class_50sample1000.npy\")\n",
    "#image_test_data = np.load(\"./MyData_40class_50sample20.npy\")\n",
    "\n",
    "image_train_data = np.load(train_file)\n",
    "image_test_data = np.load(test_file)\n",
    "\n",
    "print(image_train_data.shape)\n",
    "print(image_test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000,)\n",
      "(400000,)\n"
     ]
    }
   ],
   "source": [
    "## generate the labels for image\n",
    "labels = [np.full((IMAGE_PER_CLASS,), Classes.index(qdraw)) for qdraw in Classes]\n",
    "label_data = np.concatenate(labels,axis=0)\n",
    "\n",
    "label_train_data = label_data\n",
    "label_test_data = label_data\n",
    "\n",
    "print(label_train_data.shape)\n",
    "print(label_test_data.shape)\n",
    "#label_data.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_object(obj):\n",
    "    # Reshape 784 array into 28x28 image\n",
    "    image = obj.reshape([28,28])\n",
    "    fig, axes = plt.subplots(1, )\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    plt.imshow(image, cmap='gray_r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCD8QFOEHgvp/gwAvjUzo6csAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1b36a97b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane\n"
     ]
    }
   ],
   "source": [
    "show_object(image_train_data[25])\n",
    "print(Classes[label_train_data[25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_utils.to_categorical(label_train_data[25], NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shuffling function\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "image_train_data,label_train_data = unison_shuffled_copies(image_train_data,label_train_data)\n",
    "image_test_data,label_test_data = unison_shuffled_copies(image_test_data,label_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXHXXGy3kAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1b36abec88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand\n"
     ]
    }
   ],
   "source": [
    "show_object(image_train_data[25])\n",
    "print(Classes[label_train_data[25]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert label to one hot\n",
    "label_train_data = np_utils.to_categorical(label_train_data, NUM_CLASSES)\n",
    "label_test_data = np_utils.to_categorical(label_test_data, NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train_data[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape and normalized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train_data = image_train_data.reshape(image_train_data.shape[0], 28, 28, 1)\n",
    "image_test_data = image_test_data.reshape(image_test_data.shape[0], 28, 28, 1)\n",
    "\n",
    "image_train_data = image_train_data.astype('float32')\n",
    "image_test_data = image_test_data.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the image\n",
    "image_train_data = image_train_data/255\n",
    "image_test_data = image_test_data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXHXXGy3kAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1b23ef1e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_object(image_train_data[25]) # should still show a (pale) image,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_img_gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "#                                   height_shift_range=0.08, zoom_range=0.08)\n",
    "\n",
    "train_img_gen = ImageDataGenerator()\n",
    "test_img_gen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_img_gen.flow(image_train_data, label_train_data, batch_size=BATCH_SIZE)\n",
    "test_generator = test_img_gen.flow(image_test_data, label_test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c3d6b07e2072>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Prepare images for training in batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./My_Data40class_50sample1000.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mval_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./My_Data40class_50sample20.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_batches' is not defined"
     ]
    }
   ],
   "source": [
    "def generator(features, labels, batch_size):\n",
    "    # Create empty arrays to contain batch of features and labels#\n",
    "    batch_features = np.zeros((batch_size, 64, 64, 3))\n",
    "    batch_labels = np.zeros((batch_size,1))\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            # choose random index in features\n",
    "            index= random.choice(len(features),1)\n",
    "            batch_features[i] = some_processing(features[index])\n",
    "            batch_labels[i] = labels[index]\n",
    "        yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the training paramaters\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "Conv2d_2 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "Maxpool2d_1 (MaxPooling2D)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv2d_3 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "Maxpool2d_2 (MaxPooling2D)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 128)               401536    \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "Outputlayer (Dense)          (None, 40)                5160      \n",
      "=================================================================\n",
      "Total params: 451,400\n",
      "Trainable params: 451,336\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define the model1\n",
    "input_shape = (28, 28, 1) #1 channel, 28x28 size\n",
    "\n",
    "inputs = Input(input_shape) \n",
    "x = Conv2D(filters=32, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_1\")(inputs)\n",
    "x = BatchNormalization(axis=-1)(x)\n",
    "x = Conv2D(filters=32, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_2\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='same', name = \"Maxpool2d_1\")(x)\n",
    "\n",
    "x = Conv2D(filters=64, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_3\")(x)\n",
    "#x = BatchNormalization(axis=-1)(x)\n",
    "#x = Conv2D(filters=64, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', name = \"Conv2d_4\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='same', name = \"Maxpool2d_2\")(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='softmax', name = \"Dense_1\")(x)\n",
    "x = Dense(128, activation='softmax', name = \"Dense_2\")(x)\n",
    "outputs = Dense(NUM_CLASSES, activation='softmax', name = \"Outputlayer\")(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6250/6250 [==============================] - 777s - loss: 3.1044 - acc: 0.1466 - val_loss: 2.8636 - val_acc: 0.1672\n",
      "Epoch 2/10\n",
      "3993/6250 [==================>...........] - ETA: 272s - loss: 2.8884 - acc: 0.1641"
     ]
    }
   ],
   "source": [
    "#compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#train the model\n",
    "hist = model.fit_generator(train_generator,\n",
    "                           epochs=EPOCHS,\n",
    "                           steps_per_epoch=(IMAGE_PER_CLASS*NUM_CLASSES/BATCH_SIZE),\n",
    "                           verbose=1,\n",
    "                           max_queue_size=10,\n",
    "                           validation_data=test_generator,\n",
    "                           validation_steps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = model.evaluate_generator(test_generator,10,10,workers=1,pickle_safe=False)\n",
    "print(\"model accuracy:\",metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('Quickdraw.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 3 from 1 for 'conv2d_2/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    653\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 654\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    655\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_2/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-8b19a334609b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                 \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m             \u001b[1;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                 dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             outputs = K.conv3d(\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[0;32m   3162\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3163\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3164\u001b[1;33m         data_format='NHWC')\n\u001b[0m\u001b[0;32m   3165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_postprocess_conv2d_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution\u001b[1;34m(input, filter, padding, strides, dilation_rate, name, data_format)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mdilation_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m         op=op)\n\u001b[0m\u001b[0;32m    673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mwith_space_to_batch\u001b[1;34m(input, dilation_rate, padding, op, filter_shape, spatial_dims, data_format)\u001b[0m\n\u001b[0;32m    336\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dilation_rate must be positive\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconst_rate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_spatial_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m   \u001b[1;31m# We have two padding contributions. The first is used for converting \"SAME\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mop\u001b[1;34m(input_converted, _, padding)\u001b[0m\n\u001b[0;32m    662\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m           \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     return with_space_to_batch(\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m_non_atrous_convolution\u001b[1;34m(input, filter, padding, data_format, strides, name)\u001b[0m\n\u001b[0;32m    129\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m    132\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mconv_dims\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"NDHWC\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[0;32m    395\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[0;32m    398\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    768\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2630\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2631\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2632\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2633\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2634\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1909\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1861\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, require_shape_fn)\u001b[0m\n\u001b[0;32m    593\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    594\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m                                   require_shape_fn)\n\u001b[0m\u001b[0;32m    596\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    657\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_2/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32]."
     ]
    }
   ],
   "source": [
    "#define the model2 - sequencial\n",
    "\n",
    "input_shape = (1, 28, 28) #1 channel, 28x28 size\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "model.fit(image_train_data, labal_train_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(image_test_data, label_test_data))\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#train the model\n",
    "model.fit(image_train_data, labal_train_data,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          validation_data=(image_test_data, label_test_data))\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_object(x_data[53000])\n",
    "print(y_data[53000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
